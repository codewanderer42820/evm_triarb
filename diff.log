diff --git a/.DS_Store b/.DS_Store
index 8c8daf4..5bd0383 100644
Binary files a/.DS_Store and b/.DS_Store differ
diff --git a/ring56/pinned_consumer.go b/ring56/pinned_consumer.go
index 908ffe1..4482cb7 100644
--- a/ring56/pinned_consumer.go
+++ b/ring56/pinned_consumer.go
@@ -34,8 +34,14 @@ const (
 //
 // When the consumer exits, the `done` channel is closed for cleanup.
 //
+// Compiler directives:
+//   - nosplit: avoid stack checks, safe due to fixed size and no runtime calls inside loop
+//   - inline: encourage aggressive inlining of caller
+//   - registerparams: pass arguments in registers (Go 1.21+ ABI)
+//
 //go:nosplit
 //go:inline
+//go:registerparams
 func PinnedConsumer(
 	core int, // Logical CPU core to bind this thread to
 	ring *Ring, // Ring to consume from (SPSC)
diff --git a/ring56/relax_amd64.go b/ring56/relax_amd64.go
index f6759c7..8ca60ff 100644
--- a/ring56/relax_amd64.go
+++ b/ring56/relax_amd64.go
@@ -7,6 +7,11 @@ package ring56
 // Used in spin loops to avoid excess power draw or wasting hyperthreaded sibling resources.
 // The actual implementation is provided in relax_amd64.s.
 //
+// Compiler directives:
+//   - noescape: tells compiler this pointer-returning fn leaks no memory to heap
+//   - nosplit: suppresses stack split logic (safe for tight spin loops)
+//   - inline: strongly encourages inlining at call sites
+//
 //go:noescape
 //go:nosplit
 //go:inline
diff --git a/ring56/relax_amd64.s b/ring56/relax_amd64.s
index 0645b4a..6c85698 100644
--- a/ring56/relax_amd64.s
+++ b/ring56/relax_amd64.s
@@ -11,7 +11,6 @@
 //
 // Marked NOSPLIT to allow safe usage in nosplit Go paths.
 
-// func cpuRelax()
 TEXT ·cpuRelax(SB), NOSPLIT, $0
 	PAUSE
 	RET
diff --git a/ring56/relax_stub.go b/ring56/relax_stub.go
index 190c000..7a7df7e 100644
--- a/ring56/relax_stub.go
+++ b/ring56/relax_stub.go
@@ -12,6 +12,10 @@ package ring56
 // The function is safe to call unconditionally in spin loops — on unsupported
 // targets it simply does nothing.
 //
+// Compiler directives:
+//   - nosplit: safe to embed in spin loops
+//   - inline: encourages compiler to drop into instruction stream
+//
 //go:nosplit
 //go:inline
 func cpuRelax() {}
diff --git a/ring56/ring.go b/ring56/ring.go
index 14f0a93..5fcc125 100644
--- a/ring56/ring.go
+++ b/ring56/ring.go
@@ -18,22 +18,38 @@
 package ring56
 
 import (
-	"sync/atomic"
+	_ "sync/atomic" // only used in fallback paths
 )
 
+// slot holds one 56-byte payload and its sequence number for tracking ownership.
+//
+// Compiler directives:
+//   - notinheap: avoids GC tracking
+//   - align 64: ensures slot fits cleanly in cachelines for producer/consumer
+//
+//go:notinheap
+//go:align 64
 type slot struct {
-	val [56]byte // fixed-size payload (cache-aligned)
-	seq uint64   // slot ticket number for cursor sync
+	val [56]byte
+	seq uint64
 }
 
+// Ring is an ultra-fast, cache-friendly, single-producer single-consumer ring buffer.
+//
+// Compiler directives:
+//   - notinheap: ensures arena safety and zero GC metadata
+//   - align 64: maintains cacheline separation between head/tail
+//
+//go:notinheap
+//go:align 64
 type Ring struct {
-	_    [64]byte // cache-line isolation (consumer head)
+	_    [64]byte // consumer head cacheline
 	head uint64
 
-	_    [64]byte // cache-line isolation (producer tail)
+	_    [64]byte // producer tail cacheline
 	tail uint64
 
-	_ [64]byte // full page-line isolation
+	_ [64]byte // extra padding
 
 	mask uint64
 	step uint64
@@ -41,10 +57,16 @@ type Ring struct {
 }
 
 // New constructs a ring with power-of-two size.
-// Panics if size is not valid. Caller must ensure sizing discipline.
+// Panics if size is invalid.
+//
+// Compiler directives:
+//   - nosplit: avoids stack checks in hot code
+//   - inline: inlines construction
+//   - registerparams: pass args via registers (Go 1.21+ ABI)
 //
 //go:nosplit
 //go:inline
+//go:registerparams
 func New(size int) *Ring {
 	if size <= 0 || size&(size-1) != 0 {
 		panic("ring: size must be >0 and power of two")
@@ -61,43 +83,58 @@ func New(size int) *Ring {
 }
 
 // Push attempts to enqueue a [56]byte payload.
-// Returns false if slot is not yet ready (queue full). No backoff logic.
+// Returns false if full (slot not ready).
+//
+// Compiler directives:
+//   - nosplit: tight loop safe
+//   - inline: for zero-call hot path
+//   - registerparams: for fast param passing
 //
 //go:nosplit
 //go:inline
+//go:registerparams
 func (r *Ring) Push(val *[56]byte) bool {
 	t := r.tail
 	s := &r.buf[t&r.mask]
-	if atomic.LoadUint64(&s.seq) != t {
+	if loadAcquireUint64(&s.seq) != t {
 		return false
 	}
 	s.val = *val
-	atomic.StoreUint64(&s.seq, t+1)
+	storeReleaseUint64(&s.seq, t+1)
 	r.tail = t + 1
 	return true
 }
 
-// Pop returns the next available payload pointer.
-// If empty, returns nil. Payload is valid until overwritten.
+// Pop returns the next available payload, or nil if empty.
+//
+// Compiler directives:
+//   - nosplit: safe in spin loops
+//   - inline: inlines into waiters or fast-path
+//   - registerparams: minimal arg overhead
 //
 //go:nosplit
 //go:inline
+//go:registerparams
 func (r *Ring) Pop() *[56]byte {
 	h := r.head
 	s := &r.buf[h&r.mask]
-	if atomic.LoadUint64(&s.seq) != h+1 {
+	if loadAcquireUint64(&s.seq) != h+1 {
 		return nil
 	}
 	val := &s.val
-	atomic.StoreUint64(&s.seq, h+r.step)
+	storeReleaseUint64(&s.seq, h+r.step)
 	r.head = h + 1
 	return val
 }
 
-// PopWait spins until a value is available and returns it.
-// Uses cpuRelax() for polite spin-loop yielding.
+// PopWait blocks (spins) until a value is available.
+//
+// Compiler directives:
+//   - nosplit: safe because cpuRelax is nosplit too
+//   - registerparams: minimal call overhead
 //
 //go:nosplit
+//go:registerparams
 func (r *Ring) PopWait() *[56]byte {
 	for {
 		if p := r.Pop(); p != nil {
diff --git a/ring56/ring_atomic_amd64.go b/ring56/ring_atomic_amd64.go
index 151276e..7402aef 100644
--- a/ring56/ring_atomic_amd64.go
+++ b/ring56/ring_atomic_amd64.go
@@ -4,6 +4,7 @@
 package ring56
 
 // loadAcquireUint64 performs a load with acquire semantics from *p.
+//
 // This guarantees that any subsequent loads/stores will observe memory
 // operations that happened-before the release-store to *p by another core.
 //
@@ -11,17 +12,26 @@ package ring56
 // On TSO architectures like x86-64, this satisfies acquire ordering and
 // acts as a compiler barrier without requiring mfence.
 //
+// Compiler directives:
+//   - nosplit: usable inside tight rings
+//   - inline: emits direct MOVQ
+//
 //go:nosplit
 //go:inline
 func loadAcquireUint64(p *uint64) (v uint64)
 
 // storeReleaseUint64 stores the value v into *p with release semantics.
+//
 // This ensures that all memory writes before the store are visible to
 // another core before it observes the new value at *p.
 //
 // It is implemented using MOVQ without LOCK or MFENCE on x86-64, as
 // TSO memory ordering already preserves the required visibility guarantees.
 //
+// Compiler directives:
+//   - nosplit: no stack growth
+//   - inline: emits MOVQ directly
+//
 //go:nosplit
 //go:inline
 func storeReleaseUint64(p *uint64, v uint64)
diff --git a/ring56/ring_bench_test.go b/ring56/ring_bench_test.go
index a84dccb..53382c6 100644
--- a/ring56/ring_bench_test.go
+++ b/ring56/ring_bench_test.go
@@ -5,9 +5,9 @@
 //   - Cross-core contention simulation
 //   - Elision-resistant test design (via runtime.KeepAlive + global sink)
 //
-// All tests assume:
-//   - Ring is correctly sized (power-of-two)
-//   - Producer and consumer maintain SPSC contract
+// Assumptions:
+//   - Ring is correctly sized (power-of-two).
+//   - Producer and consumer maintain SPSC contract.
 
 package ring56
 
@@ -23,6 +23,7 @@ var dummy56 = &[56]byte{1, 2, 3} // constant payload
 var sink any                     // escape sink to prevent elision
 
 // BenchmarkRing_Push measures producer-only throughput under full ring pressure.
+// Caller spins on Pop if full, emulating best-effort batching.
 func BenchmarkRing_Push(b *testing.B) {
 	r := New(benchCap)
 	b.ReportAllocs()
@@ -36,7 +37,7 @@ func BenchmarkRing_Push(b *testing.B) {
 	}
 }
 
-// BenchmarkRing_Pop measures raw consumer throughput assuming mostly full ring.
+// BenchmarkRing_Pop measures raw consumer throughput assuming ring is preloaded.
 func BenchmarkRing_Pop(b *testing.B) {
 	r := New(benchCap)
 	for i := 0; i < benchCap-1; i++ {
@@ -57,7 +58,8 @@ func BenchmarkRing_Pop(b *testing.B) {
 	runtime.KeepAlive(sink)
 }
 
-// BenchmarkRing_PushPop measures tight producer-consumer loop in same goroutine.
+// BenchmarkRing_PushPop measures a tight in-goroutine loop of Pop→Push.
+// This approximates microservice-style relaying.
 func BenchmarkRing_PushPop(b *testing.B) {
 	r := New(benchCap)
 	for i := 0; i < benchCap/2; i++ {
@@ -74,7 +76,10 @@ func BenchmarkRing_PushPop(b *testing.B) {
 	runtime.KeepAlive(sink)
 }
 
-// BenchmarkRing_CrossCore simulates full-duplex behavior with true core separation.
+// BenchmarkRing_CrossCore simulates a full producer-consumer split across OS threads.
+//
+// GOMAXPROCS(2) gives us real core separation on most OSes.
+// This captures true inter-core communication cost through L2 or L3.
 func BenchmarkRing_CrossCore(b *testing.B) {
 	runtime.GOMAXPROCS(2)
 	r := New(benchCap)
diff --git a/ring56/setaffinity_linux.go b/ring56/setaffinity_linux.go
index 9a8cb2f..5e3eb2b 100644
--- a/ring56/setaffinity_linux.go
+++ b/ring56/setaffinity_linux.go
@@ -10,6 +10,7 @@ import (
 
 // cpuMasks holds precomputed one-word masks (bit = 1 << cpu) for CPUs 0–63.
 // Each is statically initialized so it can be passed directly to the syscall.
+// This avoids runtime heap allocations and stays entirely nosplit-friendly.
 var cpuMasks = [...][1]uintptr{
 	{1 << 0}, {1 << 1}, {1 << 2}, {1 << 3}, {1 << 4}, {1 << 5}, {1 << 6}, {1 << 7},
 	{1 << 8}, {1 << 9}, {1 << 10}, {1 << 11}, {1 << 12}, {1 << 13}, {1 << 14}, {1 << 15},
@@ -21,6 +22,15 @@ var cpuMasks = [...][1]uintptr{
 	{1 << 56}, {1 << 57}, {1 << 58}, {1 << 59}, {1 << 60}, {1 << 61}, {1 << 62}, {1 << 63},
 }
 
+// setAffinity binds the current OS thread to the specified logical CPU.
+//
+// On valid Linux platforms, this is implemented via syscall.SYS_SCHED_SETAFFINITY.
+// It uses a precomputed static mask (1<<cpu) to avoid heap allocations.
+//
+// Compiler directives:
+//   - nosplit: safe because we use static memory and make no heap calls
+//   - inline: encourages inlining into goroutine prologues like PinnedConsumer
+//
 //go:nosplit
 //go:inline
 func setAffinity(cpu int) {
diff --git a/ring56/setaffinity_stub.go b/ring56/setaffinity_stub.go
index 6072c48..eb33d27 100644
--- a/ring56/setaffinity_stub.go
+++ b/ring56/setaffinity_stub.go
@@ -9,6 +9,10 @@ package ring56
 // This allows the same API to be used across all targets without
 // conditional code in the caller — simply call unconditionally.
 //
+// Compiler directives:
+//   - nosplit: avoids stack growth for safe substitution in tight loops
+//   - inline: ensures complete elimination by inlining away as no-op
+//
 //go:nosplit
 //go:inline
 func setAffinity(cpu int) {}
